{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install h5py pandas numpy scikit-learn tensorflow keras imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from scipy.signal import butter, filtfilt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Process original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "file_part_number = 0\n",
    "excel_file_path = '/content/drive/MyDrive/DATASETS/exams.xlsx'\n",
    "excel_data = pd.read_excel(excel_file_path)\n",
    "\n",
    "hdf5_file_paths = [\n",
    "    '/content/drive/MyDrive/DATASETS/exams_part0.hdf5',\n",
    "    '/content/drive/MyDrive/DATASETS/exams_part1.hdf5',\n",
    "    '/content/drive/MyDrive/DATASETS/exams_part2.hdf5',\n",
    "    '/content/drive/MyDrive/DATASETS/exams_part3.hdf5',\n",
    "    '/content/drive/MyDrive/DATASETS/exams_part4.hdf5',\n",
    "    '/content/drive/MyDrive/DATASETS/exams_part5.hdf5',\n",
    "    '/content/drive/MyDrive/DATASETS/exams_part6.hdf5',\n",
    "    '/content/drive/MyDrive/DATASETS/exams_part7.hdf5',\n",
    "    '/content/drive/MyDrive/DATASETS/exams_part8.hdf5',\n",
    "    '/content/drive/MyDrive/DATASETS/exams_part9.hdf5',\n",
    "    '/content/drive/MyDrive/DATASETS/exams_part10.hdf5',\n",
    "    '/content/drive/MyDrive/DATASETS/exams_part11.hdf5',\n",
    "    '/content/drive/MyDrive/DATASETS/exams_part12.hdf5',\n",
    "    '/content/drive/MyDrive/DATASETS/exams_part13.hdf5',\n",
    "    '/content/drive/MyDrive/DATASETS/exams_part14.hdf5',\n",
    "    '/content/drive/MyDrive/DATASETS/exams_part15.hdf5',\n",
    "    '/content/drive/MyDrive/DATASETS/exams_part16.hdf5',\n",
    "    '/content/drive/MyDrive/DATASETS/exams_part17.hdf5'\n",
    "]\n",
    "\n",
    "hdf5_file_path = hdf5_file_paths[file_part_number]\n",
    "\n",
    "combined_data = pd.DataFrame()\n",
    "\n",
    "part_name = hdf5_file_path.split('/')[-1]  # Get the HDF5 file name (e.g., 'exams_part6.hdf5')\n",
    "\n",
    "# Load the HDF5 file\n",
    "with h5py.File(hdf5_file_path, 'r') as hdf:\n",
    "    exam_id = hdf['exam_id'][:]\n",
    "    tracings = hdf['tracings'][:]\n",
    "\n",
    "# Filter the Excel data to match the current part\n",
    "filtered_data = excel_data[excel_data['trace_file'] == part_name]\n",
    "\n",
    "# Create a DataFrame for the current HDF5 file\n",
    "df_hdf5 = pd.DataFrame({\n",
    "    'exam_id': exam_id,\n",
    "    'tracings': list(tracings)\n",
    "})\n",
    "\n",
    "# Merge the filtered Excel data with the HDF5 data on 'exam_id'\n",
    "merged_data = pd.merge(df_hdf5, filtered_data, on='exam_id')\n",
    "\n",
    "# Concatenate the merged data with the combined DataFrame\n",
    "combined_data = pd.concat([combined_data, merged_data], ignore_index=True)\n",
    "\n",
    "# Check the shape of the combined data\n",
    "print(f\"Shape of the combined dataset: {combined_data.shape}\")\n",
    "\n",
    "# Optionally, check the shape of the tracings for the first few entries\n",
    "for i in range(5):\n",
    "    print(f\"Shape of the tracings array for exam_id {combined_data['exam_id'].iloc[i]}: {combined_data['tracings'].iloc[i].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Check that if all arrhythmia columns are false ecg must be labeled normal and remove rows that don't match the condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# List of arrhythmia columns\n",
    "arrhythmia_columns = ['1dAVb', 'RBBB', 'LBBB', 'SB', 'ST', 'AF']\n",
    "\n",
    "# Condition: If all arrhythmia columns are False, then 'normal_ecg' must be True\n",
    "# Step 1: Identify rows where all arrhythmia columns are False\n",
    "arrhythmia_false_condition = combined_data[arrhythmia_columns].sum(axis=1) == 0\n",
    "\n",
    "# Step 2: Identify rows where 'normal_ecg' is False\n",
    "normal_ecg_false_condition = combined_data['normal_ecg'] == False\n",
    "\n",
    "# Step 3: Combine the conditions: we want to remove rows where all arrhythmias are False and 'normal_ecg' is also False\n",
    "violation_condition = arrhythmia_false_condition & normal_ecg_false_condition\n",
    "\n",
    "# Step 4: Filter out the rows that violate the condition\n",
    "cleaned_data = combined_data[~violation_condition]\n",
    "\n",
    "# Display the number of rows removed\n",
    "rows_removed = len(combined_data) - len(cleaned_data)\n",
    "print(f\"Number of rows removed: {rows_removed}\")\n",
    "\n",
    "# Display the shape of the cleaned dataset\n",
    "print(f\"Shape of the cleaned dataset: {cleaned_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Check that ecg does not have both an arrhythmia and the normal label, and remove ones that violate the condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Condition: If at least one arrhythmia column is True, 'normal_ecg' must be False\n",
    "# Step 1: Identify rows where at least one arrhythmia column is True\n",
    "arrhythmia_true_condition = cleaned_data[arrhythmia_columns].sum(axis=1) > 0\n",
    "\n",
    "# Step 2: Identify rows where 'normal_ecg' is True (this is invalid if an arrhythmia is present)\n",
    "normal_ecg_true_condition = cleaned_data['normal_ecg'] == True\n",
    "\n",
    "# Step 3: Combine the conditions: we want to remove rows where any arrhythmia column is True and 'normal_ecg' is also True\n",
    "violation_condition = arrhythmia_true_condition & normal_ecg_true_condition\n",
    "\n",
    "# Step 4: Filter out the rows that violate the condition\n",
    "final_cleaned_data = cleaned_data[~violation_condition]\n",
    "\n",
    "# Display the number of rows removed\n",
    "rows_removed = len(cleaned_data) - len(final_cleaned_data)\n",
    "print(f\"Number of rows removed: {rows_removed}\")\n",
    "\n",
    "# Display the shape of the cleaned dataset\n",
    "print(f\"Shape of the final cleaned dataset: {final_cleaned_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Visualize the balance of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# List of the categories (arrhythmia columns + normal ECG)\n",
    "categories = ['1dAVb', 'RBBB', 'LBBB', 'SB', 'ST', 'AF', 'normal_ecg']\n",
    "\n",
    "# Count the number of 'True' values in each category\n",
    "category_counts = final_cleaned_data[categories].sum()\n",
    "\n",
    "# Display the count for each category\n",
    "print(\"ECG counts for each category:\")\n",
    "print(category_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Drop unnecesary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['age', 'is_male', 'nn_predicted_age', 'patient_id', 'death', 'timey', 'trace_file', 'exam_id']\n",
    "final_cleaned_data = final_cleaned_data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Verify the cleaned data\n",
    "print(final_cleaned_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Remove rows where multiple arrhythmias are indicated as true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# List of arrhythmia columns (excluding 'normal_ecg')\n",
    "arrhythmia_columns = ['1dAVb', 'RBBB', 'LBBB', 'SB', 'ST', 'AF']\n",
    "\n",
    "# Count how many rows have multiple arrhythmias as True\n",
    "multiple_arrhythmias_mask = (final_cleaned_data[arrhythmia_columns].sum(axis=1) > 1)\n",
    "num_multiple_arrhythmias = multiple_arrhythmias_mask.sum()\n",
    "\n",
    "print(f\"Number of rows with multiple arrhythmias: {num_multiple_arrhythmias}\")\n",
    "\n",
    "# Drop rows where multiple arrhythmias are True\n",
    "final_cleaned_data = final_cleaned_data[~multiple_arrhythmias_mask]\n",
    "\n",
    "# Count rows where 'normal_ecg' is False\n",
    "false_normal_ecg = (final_cleaned_data['normal_ecg'] == False).sum()\n",
    "\n",
    "print(f\"Number of rows where 'normal_ecg' is False: {false_normal_ecg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Check that columns are balanced after all the manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# List of arrhythmia columns (excluding 'normal_ecg')\n",
    "arrhythmia_columns = ['1dAVb', 'RBBB', 'LBBB', 'SB', 'ST', 'AF']\n",
    "\n",
    "# Sum the total number of 'True' values across all arrhythmia columns combined\n",
    "total_arrhythmia_true_values = final_cleaned_data[arrhythmia_columns].values.sum()\n",
    "\n",
    "# Display the total\n",
    "print(f\"Total 'True' values across all arrhythmia columns: {total_arrhythmia_true_values}\")\n",
    "\n",
    "final_cleaned_data = final_cleaned_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(final_cleaned_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Save the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "destination_folder = '/content/drive/MyDrive/DATASETS/'  # Adjust to your Google Drive folder\n",
    "\n",
    "# Step 1: Save the multi-dimensional ECG tracings separately as NumPy arrays\n",
    "np.save(destination_folder + f'tracings_part_{file_part_number}.npy', final_cleaned_data['tracings'].values)\n",
    "\n",
    "# Step 2: Drop the 'tracings' column\n",
    "balanced_data = final_cleaned_data.drop(columns=['tracings'])\n",
    "\n",
    "# Step 3: Save the rest of the DataFrame to HDF5 in your Google Drive\n",
    "balanced_data.to_hdf(destination_folder + f'data_part_{file_part_number}.h5', key='df', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Combine the individual parts into training and testing sets. The training set will be further divided into training and validation data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Base path to your Google Drive dataset folder\n",
    "base_path = '/content/drive/MyDrive/DATASETS/'\n",
    "\n",
    "# Initialize an empty DataFrame to hold the combined data\n",
    "combined_data = pd.DataFrame()\n",
    "\n",
    "# Loop through file_part_numbers from 0 to 13\n",
    "for file_part_number in range(14, 18):  # 0 to 13 inclusive\n",
    "    # Define file names for the current part\n",
    "    files = [f\"tracings_part_{file_part_number}.npy\", f'data_part_{file_part_number}.h5']\n",
    "    tracings_file = base_path + files[0]\n",
    "    data_file = base_path + files[1]\n",
    "\n",
    "    # Load the current part's data\n",
    "    print(f\"Loading data for part {file_part_number}...\")\n",
    "    data = pd.read_hdf(data_file)\n",
    "    tracings = np.load(tracings_file, allow_pickle=True)\n",
    "\n",
    "    # Add the tracings back into the DataFrame\n",
    "    data['tracings'] = list(tracings)\n",
    "\n",
    "    # Append the current part's data to the combined DataFrame\n",
    "    combined_data = pd.concat([combined_data, data], ignore_index=True)\n",
    "\n",
    "    print(f\"Data for part {file_part_number} loaded and appended.\")\n",
    "\n",
    "# After the loop completes, combined_data will contain all parts\n",
    "print(\"All parts loaded and combined successfully.\")\n",
    "print(combined_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Visualize the balance of the combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# List of the categories (arrhythmia columns + normal ECG)\n",
    "categories = ['1dAVb', 'RBBB', 'LBBB', 'SB', 'ST', 'AF', 'normal_ecg']\n",
    "\n",
    "# Count the number of 'True' values in each category\n",
    "category_counts = combined_data[categories].sum()\n",
    "\n",
    "# Display the count for each category\n",
    "print(\"ECG counts for each category:\")\n",
    "print(category_counts)\n",
    "\n",
    "combined_data = combined_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(combined_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Saved the combined parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "destination_folder = '/content/drive/MyDrive/DATASETS/'  # Adjust to your Google Drive folder\n",
    "\n",
    "# Step 1: Save the multi-dimensional ECG tracings separately as NumPy arrays\n",
    "np.save(destination_folder + f'tracings_part_14to17.npy', combined_data['tracings'].values)\n",
    "\n",
    "# Step 2: Drop the 'tracings' column\n",
    "balanced_data = combined_data.drop(columns=['tracings'])\n",
    "\n",
    "# Step 3: Save the rest of the DataFrame to HDF5 in your Google Drive\n",
    "balanced_data.to_hdf(destination_folder + f'data_part_14to17.h5', key='df', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Balnce and filter combined parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Rebalance columns so as not to overfit to normal_ecg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "files = [\"tracings_part_0to13.npy\", \"data_part_0to13.h5\"]\n",
    "\n",
    "# Load data\n",
    "base_path = '/content/drive/MyDrive/DATASETS/'\n",
    "tracings_file = os.path.join(base_path, files[0])\n",
    "data_file = os.path.join(base_path, files[1])\n",
    "data = pd.read_hdf(data_file)\n",
    "tracings = np.load(tracings_file, allow_pickle=True)\n",
    "data['tracings'] = list(tracings)\n",
    "\n",
    "# List of the categories (arrhythmia columns + normal ECG)\n",
    "categories = ['1dAVb', 'RBBB', 'LBBB', 'SB', 'ST', 'AF', 'normal_ecg']\n",
    "\n",
    "# Count the number of 'True' values in each category\n",
    "category_counts = data[categories].sum()\n",
    "\n",
    "# Display the count for each category\n",
    "print(\"ECG counts for each category:\")\n",
    "print(category_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Drop a random selection of normal ecgs and visualize new balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# List of categories (arrhythmia columns + normal ECG)\n",
    "categories = ['1dAVb', 'RBBB', 'LBBB', 'SB', 'ST', 'AF', 'normal_ecg']\n",
    "\n",
    "# Separate normal_ecg entries from the rest\n",
    "normal_ecg_entries = data[data['normal_ecg'] == True]\n",
    "other_entries = data[data['normal_ecg'] == False]\n",
    "\n",
    "# Randomly sample 4500 'normal_ecg' entries\n",
    "normal_ecg_sampled = normal_ecg_entries.sample(n=4500, random_state=42)\n",
    "\n",
    "# Combine sampled normal_ecg entries with non-normal_ecg entries\n",
    "balanced_data = pd.concat([normal_ecg_sampled, other_entries], ignore_index=True)\n",
    "\n",
    "# Extract tracings and labels to ensure they are properly aligned\n",
    "X = np.array([tracing for tracing in balanced_data['tracings']])\n",
    "y = balanced_data[categories].values  # Ensure labels are extracted correctly\n",
    "\n",
    "# List of the categories (arrhythmia columns + normal ECG)\n",
    "categories = ['1dAVb', 'RBBB', 'LBBB', 'SB', 'ST', 'AF', 'normal_ecg']\n",
    "\n",
    "# Count the number of 'True' values in each category\n",
    "category_counts = balanced_data[categories].sum()\n",
    "\n",
    "# Display the count for each category\n",
    "print(\"ECG counts for each category:\")\n",
    "print(category_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Apply a highpass filter to the data to get rid of baseline drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# High-pass filter functions\n",
    "def butter_highpass(cutoff, fs, order=5):\n",
    "    nyquist = 0.5 * fs  # Nyquist Frequency\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def highpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_highpass(cutoff, fs, order=order)\n",
    "    y = filtfilt(b, a, data, axis=0)  # Apply along each lead (axis=0)\n",
    "    return y\n",
    "\n",
    "# Apply high-pass filter to all tracings\n",
    "cutoff_frequency = 1  # in Hz\n",
    "sampling_frequency = 400  # in Hz\n",
    "filter_order = 5\n",
    "\n",
    "print(\"Applying high-pass filter to the data...\")\n",
    "X_filtered = np.array([\n",
    "    highpass_filter(tracing, cutoff_frequency, sampling_frequency, order=filter_order)\n",
    "    for tracing in X\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Save the balanced and filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Reassign filtered tracings to the balanced_data DataFrame\n",
    "balanced_data['tracings'] = list(X_filtered)\n",
    "\n",
    "# Save the filtered data\n",
    "destination_folder = '/content/drive/MyDrive/RedoneDataOct/'  # Adjust as necessary\n",
    "\n",
    "# Step 1: Save the multi-dimensional ECG tracings separately as NumPy arrays\n",
    "np.save(destination_folder + 'tracings_part_14to17_filtered.npy', X_filtered)\n",
    "\n",
    "# Step 2: Drop the 'tracings' column\n",
    "balanced_data = balanced_data.drop(columns=['tracings'])\n",
    "\n",
    "# Step 3: Save the rest of the DataFrame to HDF5\n",
    "balanced_data.to_hdf(destination_folder + 'data_part_14to17_filtered.h5', key='df', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Split data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "files = [\"tracings_part_0to13_filtered.npy\", \"data_part_0to13_filtered.h5\"]\n",
    "\n",
    "base_path = '/content/drive/MyDrive/RedoneDataOct/'\n",
    "\n",
    "tracings_file = os.path.join(base_path, files[0])\n",
    "data_file = os.path.join(base_path, files[1])\n",
    "\n",
    "data = pd.read_hdf(data_file)\n",
    "tracings = np.load(tracings_file, allow_pickle=True)\n",
    "data['tracings'] = list(tracings)\n",
    "\n",
    "# Extract labels\n",
    "label_columns = ['1dAVb', 'RBBB', 'LBBB', 'SB', 'ST', 'AF', 'normal_ecg']\n",
    "\n",
    "# Define X (features) and y (labels)\n",
    "X = np.array([tracing for tracing in data['tracings']])\n",
    "y = data[['1dAVb', 'RBBB', 'LBBB', 'SB', 'ST', 'AF', 'normal_ecg']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Save the parts individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Split the data\n",
    "# Use train_test_split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 2: Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Step 3: Fit the scaler only on the training data (lead-wise)\n",
    "X_train_scaled = np.array([scaler.fit_transform(tracing) for tracing in X_train])\n",
    "\n",
    "# Step 4: Apply the same transformation to the test data using the fitted scaler\n",
    "X_test_scaled = np.array([scaler.transform(tracing) for tracing in X_test])\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, '/content/drive/MyDrive/RedoneDataOct/Processed/Generalscaler.pkl')\n",
    "\n",
    "# Define the destination folder\n",
    "destination_folder = '/content/drive/MyDrive/RedoneDataOct/Processed/'\n",
    "\n",
    "# Save X_train_scaled and X_test_scaled as NumPy arrays\n",
    "np.save(destination_folder + 'X_train_scaled.npy', X_train_scaled)\n",
    "np.save(destination_folder + 'X_test_scaled.npy', X_test_scaled)\n",
    "\n",
    "# Save y_train and y_test (these can be saved using joblib or NumPy)\n",
    "np.save(destination_folder + 'y_train.npy', y_train)\n",
    "np.save(destination_folder + 'y_test.npy', y_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
