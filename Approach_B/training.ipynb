{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C0zDjx5369O"
      },
      "source": [
        "#Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tqV-j2a536Zk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.layers import SeparableConv1D, SpatialDropout1D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tswRHJM73-A1"
      },
      "source": [
        "#Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtprkAM14B8C"
      },
      "outputs": [],
      "source": [
        "destination_folder = '/content/drive/MyDrive/RedoneDataOct/Processed/'\n",
        "\n",
        "# Load X_train_scaled, X_test_scaled, y_train, y_test from the saved files\n",
        "X_train_scaled = np.load(destination_folder + 'X_train_scaled.npy')\n",
        "X_test_scaled = np.load(destination_folder + 'X_test_scaled.npy')\n",
        "y_train = np.load(destination_folder + 'y_train.npy')\n",
        "y_test = np.load(destination_folder + 'y_test.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xyvuUVb4CXp"
      },
      "source": [
        "#Define Input Shape, Learning Rate, and Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iUV3KIF4N5q"
      },
      "outputs": [],
      "source": [
        "# Define the input shape\n",
        "input_shape = (4096, 12)\n",
        "input_shape_single_lead = (input_shape[0], 1)\n",
        "\n",
        "# Learning rate scheduler with warm-up and decay\n",
        "initial_learning_rate = 1e-3\n",
        "lr_schedule = ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=100000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True\n",
        ")\n",
        "\n",
        "# Define the arrhythmia columns (ensure this is consistent with the above labels)\n",
        "arrhythmia_columns = ['1dAVb', 'RBBB', 'LBBB', 'SB', 'ST', 'AF', 'normal_ecg']\n",
        "label_columns = ['1dAVb', 'RBBB', 'LBBB', 'SB', 'ST', 'AF', 'normal_ecg']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tiQby444WQ9"
      },
      "source": [
        "#Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pq3MPaxg4Xm9"
      },
      "outputs": [],
      "source": [
        "num_classes = len(arrhythmia_columns)\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.2):\n",
        "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    x = layers.Dense(ff_dim, activation='relu')(res)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(inputs.shape[-1])(x) \n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    return x + res  \n",
        "\n",
        "def build_model(input_shape, num_classes):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Initial Convolutional Block with SpatialDropout\n",
        "    x = SeparableConv1D(filters=64, kernel_size=3, padding='same')(inputs)\n",
        "    x = SpatialDropout1D(0.2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "    # Residual Block 1 with Dilated Convolutions\n",
        "    residual = SeparableConv1D(filters=128, kernel_size=1, padding='same')(x)\n",
        "    x = SeparableConv1D(filters=128, kernel_size=3, padding='same', dilation_rate=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = SeparableConv1D(filters=128, kernel_size=3, padding='same', dilation_rate=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Add()([x, residual])\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "    # Residual Block 2 with Dilated Convolutions\n",
        "    residual = SeparableConv1D(filters=256, kernel_size=3, padding='same')(x)\n",
        "    x = SeparableConv1D(filters=256, kernel_size=5, padding='same', dilation_rate=4)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = SeparableConv1D(filters=256, kernel_size=5, padding='same', dilation_rate=4)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Add()([x, residual])\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "    # Residual Block 3 with Dilated Convolutions\n",
        "    residual = SeparableConv1D(filters=512, kernel_size=5, padding='same')(x)\n",
        "    x = SeparableConv1D(filters=512, kernel_size=7, padding='same', dilation_rate=4)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = SeparableConv1D(filters=512, kernel_size=7, padding='same', dilation_rate=4)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Add()([x, residual])\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "    # Residual Block 4 with Dilated Convolutions\n",
        "    residual = SeparableConv1D(filters=1024, kernel_size=7, padding='same')(x)\n",
        "    x = SeparableConv1D(filters=1024, kernel_size=9, padding='same', dilation_rate=4)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = SeparableConv1D(filters=1024, kernel_size=9, padding='same', dilation_rate=4)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Add()([x, residual])\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "    # **Bidirectional LSTM**\n",
        "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.3))(x)\n",
        "\n",
        "    # Additional Transformer Encoder Block\n",
        "    x = transformer_encoder(x, head_size=64, num_heads=8, ff_dim=128, dropout=0.2)\n",
        "\n",
        "    # Global Average Pooling\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    # Dense Layers with Dropout and Regularization\n",
        "    x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.002))(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    # Output Layer\n",
        "    outputs = layers.Dense(num_classes, activation='sigmoid')(x)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "model = build_model(input_shape, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFJTZKmJ4baB"
      },
      "source": [
        "#Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utcmPHxsllLD"
      },
      "outputs": [],
      "source": [
        "# Compile the model with Adam optimizer with a fixed learning rate\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=initial_learning_rate, clipvalue=1.0),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Compute class weights based on y_train to handle class imbalance\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train.argmax(axis=1)), y=y_train.argmax(axis=1))\n",
        "class_weight_dict = {i: weight if i != label_columns.index('1dAVb') else weight * 1.4 for i, weight in enumerate(class_weights)}\n",
        "\n",
        "# Callbacks for early stopping and learning rate reduction\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
        "checkpoint_filepath = '/content/drive/MyDrive/RedoneDataOct/Multi/model_testing2.keras'\n",
        "\n",
        "#Checkpoint for saving best model after each epoch\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,  # Path to save the model\n",
        "    monitor='val_loss',            # Monitor validation loss\n",
        "    save_best_only=True,           # Only save when validation loss improves\n",
        "    mode='min',                    # Save when validation loss decreases\n",
        "    verbose=1                      # Print a message when saving\n",
        ")\n",
        "\n",
        "# Train the model with class weights and validation on the test set\n",
        "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32,\n",
        "                    validation_data=(X_test_scaled, y_test),\n",
        "                    callbacks=[early_stopping, reduce_lr, model_checkpoint],\n",
        "                    class_weight=class_weight_dict)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
