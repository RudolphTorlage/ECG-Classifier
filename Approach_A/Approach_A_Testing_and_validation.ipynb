{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach A Testing and validation\n",
    "\n",
    "### This notebook tests the model against unseen testing data, and generates performace metrics by means of Precision-recall graph, confusion matrices, classification report and kappa scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OzZPKTxHE8JJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, f1_score, multilabel_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt\n",
    "import seaborn as sns\n",
    "\n",
    "# High-pass Butterworth filter function with same parameters as HPF in training code.\n",
    "def highpass_filter(data, cutoff=0.5, fs=400, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    filtered_data = filtfilt(b, a, data, axis=0)\n",
    "    return filtered_data\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('drive/MyDrive/hpf_model_x/hpf_trained_model.keras') # Replace this path with path to your model\n",
    "\n",
    "# Load normalisation data\n",
    "train_mean = np.load('drive/MyDrive/hpf_model_x/hpf_train_mean.npy') # Replace this path with path to your training data mean\n",
    "train_std = np.load('drive/MyDrive/hpf_model_x/hpf_train_std.npy')# Replace this path with path to your training data standard deviation\n",
    "\n",
    "# Path to the new combined HDF5 file\n",
    "test_hdf5_path = 'drive/MyDrive/Combined_Undersampled_normal_14_to_17.h5'\n",
    "\n",
    "# Initialise lists for test data and labels\n",
    "test_tracings = []\n",
    "test_labels = []\n",
    "\n",
    "# Define arrhythmia columns\n",
    "arrhythmia_columns = ['1dAVb', 'RBBB', 'LBBB', 'SB', 'ST', 'AF', 'normal_ecg']\n",
    "\n",
    "# Open the combined HDF5 file and load the test data\n",
    "with h5py.File(test_hdf5_path, 'r') as f:\n",
    "    for exam_id in f.keys():\n",
    "        group = f[exam_id]\n",
    "        tracing = group['tracing'][:]\n",
    "\n",
    "        # Apply high-pass filter to the test ECG data\n",
    "        filtered_tracing = highpass_filter(tracing)\n",
    "\n",
    "        # Append the filtered tracing and labels\n",
    "        label = [group.attrs[arrhythmia] for arrhythmia in arrhythmia_columns]\n",
    "        test_tracings.append(filtered_tracing)\n",
    "        test_labels.append(label)\n",
    "\n",
    "# Convert test data to numpy arrays\n",
    "X_test_balanced = np.array(test_tracings)\n",
    "y_test_balanced = np.array(test_labels)\n",
    "\n",
    "# Normalise the test data using the training mean and standard deviation\n",
    "X_test_normalized = (X_test_balanced - train_mean) / train_std\n",
    "\n",
    "# Make predictions on the normalised test set\n",
    "y_pred = model.predict(X_test_normalized)\n",
    "\n",
    "# For binary classification (normal vs arrhythmia), calculate precision-recall and thresholds\n",
    "precision = {}\n",
    "recall = {}\n",
    "thresholds = {}\n",
    "\n",
    "for i, label in enumerate(arrhythmia_columns):\n",
    "    precision[label], recall[label], thresholds[label] = precision_recall_curve(y_test_balanced[:, i], y_pred[:, i])\n",
    "\n",
    "# Plot precision-recall curve for each class\n",
    "plt.figure(figsize=(10, 7))\n",
    "for label in arrhythmia_columns:\n",
    "    plt.plot(recall[label], precision[label], label=f'Precision-Recall curve for {label}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "# Find the optimal threshold that maximizes F1 score for each arrhythmia class\n",
    "optimal_thresholds = {}\n",
    "for i, label in enumerate(arrhythmia_columns):\n",
    "    f1_scores = 2 * (precision[label] * recall[label]) / (precision[label] + recall[label])\n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    optimal_thresholds[label] = thresholds[label][optimal_idx]\n",
    "    print(f'Optimal threshold for {label}: {optimal_thresholds[label]:.2f} (F1 score: {f1_scores[optimal_idx]:.2f})')\n",
    "\n",
    "# Now, use the optimal threshold for making predictions\n",
    "y_pred_optimal = np.zeros_like(y_pred)\n",
    "for i, label in enumerate(arrhythmia_columns):\n",
    "    y_pred_optimal[:, i] = (y_pred[:, i] > optimal_thresholds[label]).astype(int)\n",
    "\n",
    "# Evaluate model performance with the new thresholds\n",
    "print(\"Classification Report with optimal thresholds:\")\n",
    "print(classification_report(y_test_balanced, y_pred_optimal, target_names=arrhythmia_columns, zero_division=1))\n",
    "\n",
    "# Compute confusion matrices for each label\n",
    "confusion_matrices = multilabel_confusion_matrix(y_test_balanced, y_pred_optimal)\n",
    "\n",
    "# Plot confusion matrices for each class\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))  # Adjust the number of subplots based on your class count\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (label, cm) in enumerate(zip(arrhythmia_columns, confusion_matrices)):\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
    "    axes[i].set_title(f'Confusion Matrix for {label}')\n",
    "    axes[i].set_xlabel('Predicted')\n",
    "    axes[i].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lwmpop_ZclAC"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "for i, label in enumerate(arrhythmia_columns):\n",
    "    kappa = cohen_kappa_score(y_test_balanced[:, i], y_pred_optimal[:, i])\n",
    "    print(f'Cohen\\'s Kappa Score for {label}: {kappa:.2f}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
