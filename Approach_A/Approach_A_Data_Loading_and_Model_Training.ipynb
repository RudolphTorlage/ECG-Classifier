{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach A Data Loading and Model Training\n",
    "\n",
    "### This notebook contains the full machine learning pipeline used in training the models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import all libraries to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ie_tX9dgR8Ex"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scipy.signal import butter, filtfilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data preprocessing\n",
    "\n",
    "This step loads in all the testing data, passes it through a high pass filter, and normalises the data, saving the normalisation constants in numpy arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9tWN_-ySFP_"
   },
   "outputs": [],
   "source": [
    "\n",
    "# This is a list of all combined parts (traces and csv data with 80% normal_ecg undersampling)\n",
    "combined_parts = [\n",
    "    'drive/MyDrive/Combined_Part_0.h5',\n",
    "    'drive/MyDrive/Combined_Part_1.h5',\n",
    "    'drive/MyDrive/Combined_Part_2.h5',\n",
    "    'drive/MyDrive/Combined_Part_3.h5',\n",
    "    'drive/MyDrive/Combined_Part_4.h5',\n",
    "    'drive/MyDrive/Combined_Part_5.h5',\n",
    "    'drive/MyDrive/Combined_Part_6.h5',\n",
    "    'drive/MyDrive/Combined_Part_7.h5',\n",
    "    'drive/MyDrive/Combined_Part_8.h5',\n",
    "    'drive/MyDrive/Combined_Part_9.h5',\n",
    "    'drive/MyDrive/Combined_Part_10.h5',\n",
    "    'drive/MyDrive/Combined_Part_11.h5',\n",
    "    'drive/MyDrive/Combined_Part_12.h5',\n",
    "    'drive/MyDrive/Combined_Part_13.h5'\n",
    "]\n",
    "\n",
    "\n",
    "# This defines the High-pass Butterworth filter parameters\n",
    "def highpass_filter(data, cutoff=0.5, fs=400, order=4):\n",
    "\n",
    "    nyquist = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    filtered_data = filtfilt(b, a, data, axis=0)  # Apply the filter along each lead (axis=0)\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "# Here we initialise empty lists to store the data\n",
    "all_tracings = []\n",
    "all_labels = []\n",
    "\n",
    "\n",
    "\n",
    "def load_combined_data_with_filter(hdf5_files):\n",
    "\n",
    "    arrhythmias = ['1dAVb', 'RBBB', 'LBBB', 'SB', 'ST', 'AF', 'normal_ecg']\n",
    "    ecg_data = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through each HDF5 file\n",
    "    for hdf5_file in hdf5_files:\n",
    "        with h5py.File(hdf5_file, 'r') as f:\n",
    "            for exam_id in f.keys():\n",
    "                group = f[exam_id]\n",
    "                tracing = group['tracing'][:]\n",
    "\n",
    "                # Apply high-pass filter to the ECG data\n",
    "                filtered_tracing = highpass_filter(tracing)\n",
    "                ecg_data.append(filtered_tracing)\n",
    "\n",
    "                # Extract arrhythmia labels\n",
    "                label = [group.attrs[arrhythmia] for arrhythmia in arrhythmias]\n",
    "                labels.append(label)\n",
    "\n",
    "    return np.array(ecg_data), np.array(labels)\n",
    "\n",
    "# Load data from combined HDF5 files with filtering\n",
    "X, y = load_combined_data_with_filter(combined_parts)\n",
    "\n",
    "# Separate normal ECGs and arrhythmias\n",
    "normal_indices = np.where(y[:, -1] == 1)[0]  # NB! this assumes that 'normal_ecg' is the last column\n",
    "arrhythmia_indices = np.where(y[:, -1] == 0)[0]  # NB! this assumes that 'normal_ecg' is the last column\n",
    "\n",
    "# Undersample normal ECGs again by 75%\n",
    "np.random.shuffle(normal_indices)\n",
    "undersampled_normal_indices = normal_indices[:len(normal_indices) // 4]  # Keep only 25% of normal ECGs - this reduces the number of normal ecgs, preventing overfitting and massive class imbalance\n",
    "\n",
    "# Combine undersampled normal ECGs with all arrhythmias\n",
    "balanced_indices = np.concatenate([undersampled_normal_indices, arrhythmia_indices])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "np.random.shuffle(balanced_indices)\n",
    "\n",
    "# Create balanced dataset\n",
    "X_balanced = X[balanced_indices]\n",
    "y_balanced = y[balanced_indices]\n",
    "\n",
    "# Normalise the ECG tracings\n",
    "train_mean = np.mean(X_balanced, axis=0)\n",
    "train_std = np.std(X_balanced, axis=0)\n",
    "X_balanced = (X_balanced - train_mean) / train_std  # Normalise across each feature/lead\n",
    "\n",
    "# Save normalisation parameters to be used for testing purposes\n",
    "np.save('hpf_train_mean.npy', train_mean)\n",
    "np.save('hpf_train_std.npy', train_std)\n",
    "\n",
    "# Here we define the input shape\n",
    "input_shape = (4096, 12)\n",
    "\n",
    "# initialise the Learning rate scheduler with warm-up and decay\n",
    "initial_learning_rate = 1e-3\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Define the arrhythmia columns (ensure this is consistent with the above labels)\n",
    "arrhythmia_columns = ['1dAVb', 'RBBB', 'LBBB', 'SB', 'ST', 'AF', 'normal_ecg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Building the model, defining learning parameters, setting checkpoints, and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BsAPcqjLSHPg"
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Initial Convolutional Block\n",
    "    x = layers.Conv1D(filters=32, kernel_size=3, padding='same')(inputs)  # non-orig\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)  # non-orig\n",
    "    x = layers.SpatialDropout1D(0.2)(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    # Residual Block 1\n",
    "    residual = layers.Conv1D(filters=64, kernel_size=1, padding='same')(x)\n",
    "    x = layers.Conv1D(filters=64, kernel_size=3, padding='same')(x)  # non-orig\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)  # non-orig\n",
    "    x = layers.Conv1D(filters=64, kernel_size=3, padding='same')(x)  # non-orig\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)  # non-orig\n",
    "    x = layers.Add()([x, residual])\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    # Additional Residual Block (New Block)\n",
    "    residual = layers.Conv1D(filters=128, kernel_size=1, padding='same')(x)\n",
    "    x = layers.Conv1D(filters=128, kernel_size=3, padding='same')(x)  # non-orig\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)  # non-orig\n",
    "    x = layers.Conv1D(filters=128, kernel_size=3, padding='same')(x)  # non-orig\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)  # non-orig\n",
    "    x = layers.Add()([x, residual])\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    # Additional Convolutional Block before LSTM\n",
    "    x = layers.Conv1D(filters=128, kernel_size=3, padding='same')(x)  # non-orig\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)  # non-orig\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    # First LSTM Block\n",
    "    x = layers.Bidirectional(layers.LSTM(32, return_sequences=True, dropout=0.3))(x)\n",
    "\n",
    "    # Second LSTM Block (New LSTM Layer)\n",
    "    x = layers.Bidirectional(layers.LSTM(32, return_sequences=True, dropout=0.3))(x)\n",
    "\n",
    "    # Multi-head Attention Block\n",
    "    attention = layers.MultiHeadAttention(num_heads=2, key_dim=16)(x, x)\n",
    "\n",
    "    # Project attention output to match LSTM output dimensions\n",
    "    attention = layers.Dense(64)(attention)\n",
    "\n",
    "    # Correct shape alignment for addition\n",
    "    x = layers.Add()([x, attention])\n",
    "\n",
    "    # Global Max Pooling\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "    # Dense Layers with Dropout and Regularisation\n",
    "    x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Output Layer\n",
    "    outputs = layers.Dense(len(arrhythmia_columns), activation='sigmoid')(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(input_shape)\n",
    "\n",
    "# Compile the model with Adam optimiser with a fixed learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=initial_learning_rate, clipvalue=1.0),\n",
    "              loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Calculate class weights to handle class imbalance and penalise incorrect 1dAVb predictions more harshly.\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_balanced.argmax(axis=1)), y=y_balanced.argmax(axis=1))\n",
    "class_weight_dict = {i: weight if i != arrhythmia_columns.index('1dAVb') else weight * 1.5 for i, weight in enumerate(class_weights)} #this made a big difference\n",
    "\n",
    "\n",
    "# Callbacks for early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "#Checkpoint for saving best model after each epoch\n",
    "checkpoint_filepath = 'hpf_best_model_checkpoint.keras'\n",
    "# This creates a ModelCheckpoint callback to save the best model during training\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,  # Path to save the model\n",
    "    monitor='val_loss',            # Monitor validation loss\n",
    "    save_best_only=True,           # Only save when validation loss improves\n",
    "    mode='min',                    # Save when validation loss decreases\n",
    "    verbose=1                      # Print a message when saving\n",
    ")\n",
    "\n",
    "# Train the model with class weights\n",
    "model.fit(X_balanced, y_balanced, epochs=100, batch_size=32, validation_split=0.2,\n",
    "          callbacks=[early_stopping, reduce_lr, model_checkpoint], class_weight=class_weight_dict)\n",
    "# Save the model and weights\n",
    "model.save('hpf_trained_model.keras')\n",
    "model.save('hpf_trained_model.h5')\n",
    "model.save_weights('hpf_model_weights.weights.h5')\n",
    "\n",
    "# Save normalisation data to a text file\n",
    "with open('hpf_normalization_data.txt', 'w') as f:\n",
    "    f.write(f'Mean: {train_mean}\\n')\n",
    "    f.write(f'Std: {train_std}\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
