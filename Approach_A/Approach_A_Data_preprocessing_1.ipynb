{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "319b316c-8d51-44b2-9eb6-91b994687654",
   "metadata": {},
   "source": [
    "# Approach A Data Preprocessing step 1:\n",
    "This notebook removes cases where all arrythmias are false but normal ecg is also false.\n",
    "It produces smaller sized .hdf5 files and corresponding csv files with patient information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fecf706a-9a9d-455d-8362-992f80c8a39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered CSV file 'Part_3_cleaned_data.csv' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Here we load the original CSV file\n",
    "csv_path = 'exams.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# This filters rows where trace_file is 'exams_partX.hdf5'\n",
    "df_filtered = df[df['trace_file'] == 'exams_partX.hdf5']\n",
    "\n",
    "# Here we select the relevant columns\n",
    "columns_to_keep = ['exam_id', '1dAVb', 'RBBB', 'LBBB', 'SB', 'ST', 'AF', 'normal_ecg', 'trace_file']\n",
    "df_filtered = df_filtered[columns_to_keep]\n",
    "\n",
    "# This further filters to exclude rows where all arrhythmia columns are False but normal_ecg is also False\n",
    "# Creating a mask for arrhythmias being all False\n",
    "arrhythmia_columns = ['1dAVb', 'RBBB', 'LBBB', 'SB', 'ST', 'AF']\n",
    "arrhythmias_all_false = (df_filtered[arrhythmia_columns] == False).all(axis=1)\n",
    "\n",
    "# create a mask for normal_ecg being False\n",
    "normal_ecg_false = df_filtered['normal_ecg'] == False\n",
    "\n",
    "# Exclude rows where all arrhythmias are False and normal_ecg is also False\n",
    "df_filtered = df_filtered[~(arrhythmias_all_false & normal_ecg_false)]\n",
    "\n",
    "# This saves the filtered data to a new CSV file\n",
    "df_filtered.to_csv('Part_X_cleaned_data.csv', index=False)\n",
    "\n",
    "print(\"Filtered CSV file 'Part_X_cleaned_data.csv' has been created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01c5698-286c-4d3e-b46a-bdbe908bef88",
   "metadata": {},
   "source": [
    "The below code is an intermediary step to count the number of arrhythmias and normal rhythms remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0387919a-e935-46fe-9dcc-9210c45e799c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of normal ECGs: 7670\n",
      "\n",
      "Number of each arrhythmia:\n",
      "1dAVb    336\n",
      "RBBB     563\n",
      "LBBB     347\n",
      "SB       319\n",
      "ST       453\n",
      "AF       415\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This loads the filtered CSV file\n",
    "cleaned_csv_path = 'Part_X_cleaned_data.csv'\n",
    "df_cleaned = pd.read_csv(cleaned_csv_path)\n",
    "\n",
    "# This counts the number of normal ECGs\n",
    "num_normal_ecgs = df_cleaned['normal_ecg'].sum()\n",
    "\n",
    "# THis counts the number of occurrences for each arrhythmia\n",
    "arrhythmia_counts = df_cleaned[['1dAVb', 'RBBB', 'LBBB', 'SB', 'ST', 'AF']].sum()\n",
    "\n",
    "# THis prints the results\n",
    "print(f\"Number of normal ECGs: {num_normal_ecgs}\")\n",
    "print(\"\\nNumber of each arrhythmia:\")\n",
    "print(arrhythmia_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ced7a5-f7f6-4bd0-ac23-c9d6fbfb5283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered HDF5 file 'Part_3_filtered_data.hdf5' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# THis loads the cleaned CSV file to get the relevant exam_ids\n",
    "cleaned_csv_path = 'Part_X_cleaned_data.csv'\n",
    "df_cleaned = pd.read_csv(cleaned_csv_path)\n",
    "filtered_exam_ids = set(df_cleaned['exam_id'].values)  # Convert to set for faster lookup\n",
    "\n",
    "# This opens the original HDF5 file and create a new HDF5 file for filtered data\n",
    "original_hdf5_path = 'exams_partX.hdf5'\n",
    "filtered_hdf5_path = 'Part_X_filtered_data.hdf5'\n",
    "\n",
    "with h5py.File(original_hdf5_path, 'r') as original_file, h5py.File(filtered_hdf5_path, 'w') as filtered_file:\n",
    "    # This reads the datasets from the original file\n",
    "    original_exam_ids = original_file['exam_id'][:]\n",
    "    original_tracings = original_file['tracings'][:]\n",
    "    \n",
    "    # this finds indices of the exam_ids that are in the filtered_exam_ids\n",
    "    indices_to_keep = [i for i, exam_id in enumerate(original_exam_ids) if exam_id in filtered_exam_ids]\n",
    "    \n",
    "    # these indices are used to to filter the data\n",
    "    filtered_exam_ids = original_exam_ids[indices_to_keep]\n",
    "    filtered_tracings = original_tracings[indices_to_keep, :, :]\n",
    "    \n",
    "    # This create datasets in the new HDF5 file with the filtered data\n",
    "    filtered_file.create_dataset('exam_id', data=filtered_exam_ids)\n",
    "    filtered_file.create_dataset('tracings', data=filtered_tracings)\n",
    "\n",
    "print(f\"Filtered HDF5 file '{filtered_hdf5_path}' has been created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305ac6c0-e395-4516-9293-3ca46d0a4482",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
